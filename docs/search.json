[
  {
    "objectID": "posts/05-p-value/index.html",
    "href": "posts/05-p-value/index.html",
    "title": "Mini-Project 5",
    "section": "",
    "text": "Dean Brooker Mini-Project 5\n\nTowards the end of Section 1, the authors say “As ‘statistical significance’ is used less, statistical thinking will be used more.” Elaborate on what you think the authors mean. Give some examples of what you think embodies “statistical thinking.”\n\nWhat I think the author means is the idea that as people move away from saying whether a finding is relevant will no longer be simply judged by it simply passing a p value test, but the research will be judged by whether the researcher can argue their methods and findings in a way that proves a point. An example of statistical thinking in my eyes is if the researcher gets a p value of 0.1, they will be able to argue why it is 0.1 and not below 0.05, and why this research is still relevant even if the p value is 0.1.An example could be, a new improvement to squash balls increased rallies by 15 seconds per rally on average but the p value is 0.07, then the statistical thinking should look more into this, maybe get more data, rerun the study. Just because it failed, doesn’t mean there isn’t something there.\n\nSection 2, third paragraph: The authors state “A label of statistical significance adds nothing to what is already conveyed by the value of p; in fact, this dichotomization of p-values makes matters worse.” Elaborate on what you think the authors means.\n\nWhat I think the author is trying to get at is that a p value being said to be significant and proving something does not mean that it is true. A very low p value just means the result is very rare if there were no real effects so researchers should use thinking to convince the reader that the research shows something important rather than hiding behind a p value just to prove what they studying has an effect. The author is saying that when people say 0.049 is significant and 0.051 is not significant, the problem is just being made worse because researchers have something to hide behind when judged. Someone could say, “I don’t think your research on whether carrots help you see in the dark proves anything, a researchers can simply say, “Its significant” and the current scientist community will back up the researcher and not be curious like the person challenging the researcher.\n\nSection 2, end of first column: The authors state “For the integrity of scientific publishing and research dissemination, therefore, whether a p-value passes any arbitrary threshold should not be considered at all when deciding which results to present or highlight.” Do you agree or disagree? How should it be decided which results to present/highlight in scientific publishing?\n\nI do not think p values should be, the be all and end all. Last semester for my econometrics project, we investigated whether EV cars has reduced carbon emissions in the US. The obvious and truthful answer is absolutely no. There are so few EV cars in the US, it is just really not having an effect yet on reducing carbon emissions in the US by state. But in our research we changed the variable of research to EV/per-capita in each state and with that, that variable was significant at a 0.05 level. Therefore, we could write a paper that portrayed on a per capita basis, EV cars are already influencing reducing carbon emissions. What I am trying to say is, if I, a very terrible academic could “create” significance, a great academic with more time could easily create significance out of nothing. The p value should not be the reason academic research should be highlighted\nThe way I think research should be presented, is if the researcher themselves pave the way for future of research in the field and not by proving something. For example, I think a researcher who says, “I have proved cursive writing improves attention to detail in children” should not be highlighted but the researcher that says, “Cursive writing improves attention to detail in kids, through my extensive research, more research and areas of interest in this topic is whether this increased attention to detail decreases creativity in children. I think research should highlighted and published if its sole purpose is to pave the future and not give “significance” and recognition to a researcher.\n4)Section 3, end of page 2: The authors state “The statistical community has not yet converged on a simple paradigm for the use of statistical inference in scientific research – and in fact it may never do so. A one-size-fits-all approach to statistical inference is an inappropriate expectation.” Do you agree or disagree? Explain.\nYes I agree with this. I agree that we should move away from the one size fits all. Different research should use different methods to decide on significance. With data from hospitals as discussed, often with large amounts of data it is easy to get a small p value but maybe with another method that is discussed, like false positive risk or analysis or credibility, a fuller picture can be portrayed. I do not know anything about those methods and that is sad in my eyes. Reading about all these other methods only in my senior year of college is sad and says something about academia. Academia is so pushed in the one size fits all, us college students aren’t getting the opportunity to be exposed to anything but the p value. If we start teaching undergraduates about other methods, maybe the next generation can be more thoughtful in statistical inference.\n5) Section 3.2: The authors note that they are envisioning “a sort of ‘statistical thoughtfulness’.” What do you think “statistical thoughtfulness” means? What are some ways to demonstrate “statistical thoughtfulness” in an analysis?\nI really liked the idea that, thoughtful research means being cautious with language and avoiding overconfident terms like “significant”. Often a paper says, for certain this is what is happening and then 3 years later another paper shows the previous paper was wrong. The first paper could have signs that they know they were wrong but they used overconfident language and try make their research sound more significant just to get published or highlighted. This paper argues that statistical thoughtfulness is presenting info not just so they can get published. An example would be clearly explaining what the confidence interval tells you and what it doesn’t tell us. It is presenting a picture and also explaining what picture it is not telling.\n6) Section 3.2.4: A few of the authors of papers in this special issue argue that some of the terminology used in statistics, such as “significance” and “confidence” can be misleading, and they propose the use of “compatibility” instead. What you do you think they believe the problem is? Do you agree or disagree (that there is a problem and that changing the name will help)?\nThey think the problem currently is “confidence” and significance is too strong of language. Significant means important and confidence sounds too certain.\nI do not agree with this really. I think just changing the wording is not at the root problem. Yes, I understand, all these suggestions paired with each other, moves the statistics community forward but I think changing language is low on the list. I think, this suggestion is too old school of thought, like that it is language that is the problem, but I think it is the method, data and inference, which is more of the problem\n7) Find a quote or point that really strikes you (i.e., made you think). What is the quote (and tell me where to find it), and why does it stand out to you?\n“Edgeworth’s (1885) original intention for statistical significance was simply as a tool to indicate when a result warrants further scrutiny.”\nFrom a bit of reading online, it appears that Edgeworth introduced many early ideas about the use of probability and inference in statistics. He argues that statistical significance should be a signal for further scrutiny and investigation and not a final decision. I was very interested in Edgeworth so I looked him up a bit. It was difficult to find stuff. Playing around with ChatGPT, it said that, Edgeworth believed that different problems may require different statistical tools and probability should be viewed as a tool for reasoned judgement. I could not find the sources, AI was referring to but if this info is true, it is incredible to see that someone so long ago, was saying exactly what this author was saying."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Final_Portfolio_Dean",
    "section": "",
    "text": "Mini-Project-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nindex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMini Project 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMini-Project-3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMini-Project-4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMini-Project 5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMini-Reflect\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMini-Project 5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Mini-Project 5",
    "section": "",
    "text": "Dean Brooker Mini-Project 5\n\nTowards the end of Section 1, the authors say “As ‘statistical significance’ is used less, statistical thinking will be used more.” Elaborate on what you think the authors mean. Give some examples of what you think embodies “statistical thinking.”\n\nWhat I think the author means is the idea that as people move away from saying whether a finding is relevant will no longer be simply judged by it simply passing a p value test, but the research will be judged by whether the researcher can argue their methods and findings in a way that proves a point. An example of statistical thinking in my eyes is if the researcher gets a p value of 0.1, they will be able to argue why it is 0.1 and not below 0.05, and why this research is still relevant even if the p value is 0.1.An example could be, a new improvement to squash balls increased rallies by 15 seconds per rally on average but the p value is 0.07, then the statistical thinking should look more into this, maybe get more data, rerun the study. Just because it failed, doesn’t mean there isn’t something there.\n\nSection 2, third paragraph: The authors state “A label of statistical significance adds nothing to what is already conveyed by the value of p; in fact, this dichotomization of p-values makes matters worse.” Elaborate on what you think the authors means.\n\nWhat I think the author is trying to get at is that a p value being said to be significant and proving something does not mean that it is true. A very low p value just means the result is very rare if there were no real effects so researchers should use thinking to convince the reader that the research shows something important rather than hiding behind a p value just to prove what they studying has an effect. The author is saying that when people say 0.049 is significant and 0.051 is not significant, the problem is just being made worse because researchers have something to hide behind when judged. Someone could say, “I don’t think your research on whether carrots help you see in the dark proves anything, a researchers can simply say, “Its significant” and the current scientist community will back up the researcher and not be curious like the person challenging the researcher.\n\nSection 2, end of first column: The authors state “For the integrity of scientific publishing and research dissemination, therefore, whether a p-value passes any arbitrary threshold should not be considered at all when deciding which results to present or highlight.” Do you agree or disagree? How should it be decided which results to present/highlight in scientific publishing?\n\nI do not think p values should be, the be all and end all. Last semester for my econometrics project, we investigated whether EV cars has reduced carbon emissions in the US. The obvious and truthful answer is absolutely no. There are so few EV cars in the US, it is just really not having an effect yet on reducing carbon emissions in the US by state. But in our research we changed the variable of research to EV/per-capita in each state and with that, that variable was significant at a 0.05 level. Therefore, we could write a paper that portrayed on a per capita basis, EV cars are already influencing reducing carbon emissions. What I am trying to say is, if I, a very terrible academic could “create” significance, a great academic with more time could easily create significance out of nothing. The p value should not be the reason academic research should be highlighted\nThe way I think research should be presented, is if the researcher themselves pave the way for future of research in the field and not by proving something. For example, I think a researcher who says, “I have proved cursive writing improves attention to detail in children” should not be highlighted but the researcher that says, “Cursive writing improves attention to detail in kids, through my extensive research, more research and areas of interest in this topic is whether this increased attention to detail decreases creativity in children. I think research should highlighted and published if its sole purpose is to pave the future and not give “significance” and recognition to a researcher.\n4)Section 3, end of page 2: The authors state “The statistical community has not yet converged on a simple paradigm for the use of statistical inference in scientific research – and in fact it may never do so. A one-size-fits-all approach to statistical inference is an inappropriate expectation.” Do you agree or disagree? Explain.\nYes I agree with this. I agree that we should move away from the one size fits all. Different research should use different methods to decide on significance. With data from hospitals as discussed, often with large amounts of data it is easy to get a small p value but maybe with another method that is discussed, like false positive risk or analysis or credibility, a fuller picture can be portrayed. I do not know anything about those methods and that is sad in my eyes. Reading about all these other methods only in my senior year of college is sad and says something about academia. Academia is so pushed in the one size fits all, us college students aren’t getting the opportunity to be exposed to anything but the p value. If we start teaching undergraduates about other methods, maybe the next generation can be more thoughtful in statistical inference.\n5) Section 3.2: The authors note that they are envisioning “a sort of ‘statistical thoughtfulness’.” What do you think “statistical thoughtfulness” means? What are some ways to demonstrate “statistical thoughtfulness” in an analysis?\nI really liked the idea that, thoughtful research means being cautious with language and avoiding overconfident terms like “significant”. Often a paper says, for certain this is what is happening and then 3 years later another paper shows the previous paper was wrong. The first paper could have signs that they know they were wrong but they used overconfident language and try make their research sound more significant just to get published or highlighted. This paper argues that statistical thoughtfulness is presenting info not just so they can get published. An example would be clearly explaining what the confidence interval tells you and what it doesn’t tell us. It is presenting a picture and also explaining what picture it is not telling.\n6) Section 3.2.4: A few of the authors of papers in this special issue argue that some of the terminology used in statistics, such as “significance” and “confidence” can be misleading, and they propose the use of “compatibility” instead. What you do you think they believe the problem is? Do you agree or disagree (that there is a problem and that changing the name will help)?\nThey think the problem currently is “confidence” and significance is too strong of language. Significant means important and confidence sounds too certain.\nI do not agree with this really. I think just changing the wording is not at the root problem. Yes, I understand, all these suggestions paired with each other, moves the statistics community forward but I think changing language is low on the list. I think, this suggestion is too old school of thought, like that it is language that is the problem, but I think it is the method, data and inference, which is more of the problem\n7) Find a quote or point that really strikes you (i.e., made you think). What is the quote (and tell me where to find it), and why does it stand out to you?\n“Edgeworth’s (1885) original intention for statistical significance was simply as a tool to indicate when a result warrants further scrutiny.”\nFrom a bit of reading online, it appears that Edgeworth introduced many early ideas about the use of probability and inference in statistics. He argues that statistical significance should be a signal for further scrutiny and investigation and not a final decision. I was very interested in Edgeworth so I looked him up a bit. It was difficult to find stuff. Playing around with ChatGPT, it said that, Edgeworth believed that different problems may require different statistical tools and probability should be viewed as a tool for reasoned judgement. I could not find the sources, AI was referring to but if this info is true, it is incredible to see that someone so long ago, was saying exactly what this author was saying."
  },
  {
    "objectID": "posts/03-Mini/index.html",
    "href": "posts/03-Mini/index.html",
    "title": "Mini-Project-3",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tibble)\n\n\np=0.1\n\ngenerate_samp_prop &lt;- function(n,p) {\n  \n  x &lt;- rbinom(1, n, p) # randomly generate number of successes for the sample\n  \n  ## number of successes divided by sample size\n  phat &lt;- x / n\n  lb &lt;-phat -2.132*sqrt(phat* (1-phat)/n)\n  ub &lt;-phat +2.132*sqrt(phat* (1-phat)/n)\n  \n  prop_df &lt;-tibble(phat,lb,ub)\n  return(prop_df)\n}\ngenerate_samp_prop(n=5, p=0.1)\n\n# A tibble: 1 × 3\n   phat     lb    ub\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1   0.2 -0.181 0.581\n\nn_sim &lt;-5000\n\nprop_ci_df &lt;-map(1:n_sim, \\ (i) generate_samp_prop(n=5, p =0.1)) |&gt; bind_rows()\n\nprop_ci_df &lt;- prop_ci_df |&gt; mutate(ci_width = ub - lb,\n                                   ci_cover_ind = if_else(p &gt; lb & p &lt; ub,\n                                                          true = 1, \n                                                          false = 0))\n\nprop_ci_df |&gt; summarise(avg_width = mean(ci_width),\n                        coverage_rate = mean(ci_cover_ind))\n\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.329         0.402\n\n\n\np=0.1\ngenerate_samp_prop &lt;- function(n,p) {\n  \n  x &lt;- rbinom(1, n, p) # randomly generate number of successes for the sample\n  \n  ## number of successes divided by sample size\n  phat &lt;- x / n\n  lb &lt;-phat -2.132*sqrt(phat* (1-phat)/n)\n  ub &lt;-phat +2.132*sqrt(phat* (1-phat)/n)\n  \n  prop_df &lt;-tibble(phat,lb,ub)\n  return(prop_df)\n}\ngenerate_samp_prop(n=5, p=0.1)\n\n# A tibble: 1 × 3\n   phat    lb    ub\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0     0     0\n\nn_sim &lt;-5000\n\nprop_ci_df &lt;-map(1:n_sim, \\ (i) generate_samp_prop(n=5, p =0.1)) |&gt; bind_rows()\n\nprop_ci_df &lt;- prop_ci_df |&gt; mutate(ci_width = ub - lb,\n                                   ci_cover_ind = if_else(p &gt; lb & p &lt; ub,\n                                                          true = 1, \n                                                          false = 0))\n\nprop_ci_df |&gt; summarise(avg_width = mean(ci_width),\n                        coverage_rate = mean(ci_cover_ind))\n\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.333         0.409\n\n\n\np=0.55\ngenerate_samp_prop &lt;- function(n,p) {\n  \n  x &lt;- rbinom(1, n, p) # randomly generate number of successes for the sample\n  \n  ## number of successes divided by sample size\n  phat &lt;- x / n\n  lb &lt;-phat -2.132*sqrt(phat* (1-phat)/n)\n  ub &lt;-phat +2.132*sqrt(phat* (1-phat)/n)\n  \n  prop_df &lt;-tibble(phat,lb,ub)\n  return(prop_df)\n}\ngenerate_samp_prop(n=5, p=0.55)\n\n# A tibble: 1 × 3\n   phat    lb    ub\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1   0.8 0.419  1.18\n\nn_sim &lt;-5000\n\nprop_ci_df &lt;-map(1:n_sim, \\ (i) generate_samp_prop(n=5, p =0.55)) |&gt; bind_rows()\n\nprop_ci_df &lt;- prop_ci_df |&gt; mutate(ci_width = ub - lb,\n                                   ci_cover_ind = if_else(p &gt; lb & p &lt; ub,\n                                                          true = 1, \n                                                          false = 0))\n\nprop_ci_df |&gt; summarise(avg_width = mean(ci_width),\n                        coverage_rate = mean(ci_cover_ind))\n\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.817         0.934\n\n\n\np=0.1\ngenerate_samp_prop &lt;- function(n,p) {\n  \n  x &lt;- rbinom(1, n, p) # randomly generate number of successes for the sample\n  \n  ## number of successes divided by sample size\n  phat &lt;- x / n\n  lb &lt;-phat -1.684*sqrt(phat* (1-phat)/n)\n  ub &lt;-phat +1.6842*sqrt(phat* (1-phat)/n)\n  \n  prop_df &lt;-tibble(phat,lb,ub)\n  return(prop_df)\n}\ngenerate_samp_prop(n=40, p=0.1)\n\n# A tibble: 1 × 3\n   phat       lb    ub\n  &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1  0.05 -0.00803 0.108\n\nn_sim &lt;-5000\n\nprop_ci_df &lt;-map(1:n_sim, \\ (i) generate_samp_prop(n=40, p =0.1)) |&gt; bind_rows()\n\nprop_ci_df &lt;- prop_ci_df |&gt; mutate(ci_width = ub - lb,\n                                   ci_cover_ind = if_else(p &gt; lb & p &lt; ub,\n                                                          true = 1, \n                                                          false = 0))\n\nprop_ci_df |&gt; summarise(avg_width = mean(ci_width),\n                        coverage_rate = mean(ci_cover_ind))\n\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.152         0.902\n\n\n\np=0.55\ngenerate_samp_prop &lt;- function(n,p) {\n  \n  x &lt;- rbinom(1, n, p) # randomly generate number of successes for the sample\n  \n  ## number of successes divided by sample size\n  phat &lt;- x / n\n  lb &lt;-phat -1.684*sqrt(phat* (1-phat)/n)\n  ub &lt;-phat +1.6842*sqrt(phat* (1-phat)/n)\n  \n  prop_df &lt;-tibble(phat,lb,ub)\n  return(prop_df)\n}\ngenerate_samp_prop(n=40, p=0.55)\n\n# A tibble: 1 × 3\n   phat    lb    ub\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 0.575 0.443 0.707\n\nn_sim &lt;-5000\n\nprop_ci_df &lt;-map(1:n_sim, \\ (i) generate_samp_prop(n=40, p =0.55)) |&gt; bind_rows()\n\nprop_ci_df &lt;- prop_ci_df |&gt; mutate(ci_width = ub - lb,\n                                   ci_cover_ind = if_else(p &gt; lb & p &lt; ub,\n                                                          true = 1, \n                                                          false = 0))\n\nprop_ci_df |&gt; summarise(avg_width = mean(ci_width),\n                        coverage_rate = mean(ci_cover_ind))\n\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.261         0.886\n\n\n\np=0.1\ngenerate_samp_prop &lt;- function(n,p) {\n  \n  x &lt;- rbinom(1, n, p) # randomly generate number of successes for the sample\n  \n  ## number of successes divided by sample size\n  phat &lt;- x / n\n  lb &lt;-phat -1.645*sqrt(phat* (1-phat)/n)\n  ub &lt;-phat +1.645*sqrt(phat* (1-phat)/n)\n  \n  prop_df &lt;-tibble(phat,lb,ub)\n  return(prop_df)\n}\ngenerate_samp_prop(n=300, p=0.1)\n\n# A tibble: 1 × 3\n    phat     lb    ub\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 0.0967 0.0686 0.125\n\nn_sim &lt;-5000\n\nprop_ci_df &lt;-map(1:n_sim, \\ (i) generate_samp_prop(n=300, p =0.1)) |&gt; bind_rows()\n\nprop_ci_df &lt;- prop_ci_df |&gt; mutate(ci_width = ub - lb,\n                                   ci_cover_ind = if_else(p &gt; lb & p &lt; ub,\n                                                          true = 1, \n                                                          false = 0))\n\nprop_ci_df |&gt; summarise(avg_width = mean(ci_width),\n                        coverage_rate = mean(ci_cover_ind))\n\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      &lt;dbl&gt;         &lt;dbl&gt;\n1    0.0567         0.888\n\n\n\np=0.55\ngenerate_samp_prop &lt;- function(n,p) {\n  \n  x &lt;- rbinom(1, n, p) # randomly generate number of successes for the sample\n  \n  ## number of successes divided by sample size\n  phat &lt;- x / n\n  lb &lt;-phat -1.645*sqrt(phat* (1-phat)/n)\n  ub &lt;-phat +1.645*sqrt(phat* (1-phat)/n)\n  \n  prop_df &lt;-tibble(phat,lb,ub)\n  return(prop_df)\n}\ngenerate_samp_prop(n=300, p=0.55)\n\n# A tibble: 1 × 3\n   phat    lb    ub\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  0.57 0.523 0.617\n\nn_sim &lt;-5000\n\nprop_ci_df &lt;-map(1:n_sim, \\ (i) generate_samp_prop(n=300, p =0.55)) |&gt; bind_rows()\n\nprop_ci_df &lt;- prop_ci_df |&gt; mutate(ci_width = ub - lb,\n                                   ci_cover_ind = if_else(p &gt; lb & p &lt; ub,\n                                                          true = 1, \n                                                          false = 0))\n\nprop_ci_df |&gt; summarise(avg_width = mean(ci_width),\n                        coverage_rate = mean(ci_cover_ind))\n\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      &lt;dbl&gt;         &lt;dbl&gt;\n1    0.0943         0.892\n\n\n|  |         | $n = $5 | $n = $40 | $n = $300 |\n|:----:|:-----------------:|:-------------:|:------------:|:------------:|\n| $p = 0.1$   | Coverage Rate       | 0.4   |  0.9072     |  0.8838         |\n| $p = 0.55$   | Coverage Rate       |0.93     |0.8846   | 0.897    |\n|    |                     |               |              |              |\n| $p = 0.1$    | Average Width        |0.332   |  0.152  |   0.057       |\n| $p = 0.55$    | Average Width        |0.817  |  0.262     |  0.094      |\n\n\n: Table of Results {.striped .hover}"
  },
  {
    "objectID": "posts/04-Bayez/index.html",
    "href": "posts/04-Bayez/index.html",
    "title": "Mini-Project-4",
    "section": "",
    "text": "##This project will attempt to answer the question of what is the probability\n##Nadla wins a point on his own serve against Novak Djokovic,at the French Open ?\n\n##To answer this question, we will use bayesian analysis where we will make some\n##informed assumptions paired with availiable data to create a probability that, \n##Nadal will wins a point on his own serve against Djokovic, at the French Open\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nps &lt;- seq(0.1, 200, length.out = 5000)\n\n\n##non-informed prior\n##beta(1,1)\nnoninformative_alpha &lt;- 1\nnoninformative_beta &lt;- 1\n\nnoninformative_prior &lt;- dbeta(ps,\n                              noninformative_alpha, noninformative_beta)\n\n##Informative Prior with Match Stats\nmatch_mean &lt;- 46 / 66\nmatch_se &lt;- 0.05657\nmatch_var &lt;- match_se^2\n\ntarget_mean &lt;- match_mean\ntarget_var &lt;- match_var\n\ninformative_alphas &lt;- seq(0.1, 200, length.out = 5000)\ninformative_betas &lt;- (informative_alphas * (1 - target_mean)) / target_mean\ninformative_vars &lt;- (informative_alphas * informative_betas) / ((informative_alphas + informative_betas)^2 * (informative_alphas + informative_betas + 1))\n\nparam_df &lt;- tibble(informative_alphas, informative_betas, informative_vars) |&gt;\n  mutate(dist_to_target = abs(informative_vars - target_var))\n\nbest_params &lt;- param_df |&gt; filter(dist_to_target == min(dist_to_target))\n\ninformative_alpha_match &lt;- best_params$informative_alphas\ninformative_beta_match &lt;- best_params$informative_betas\n\ninformative_alpha_match\n\n[1] 45.28644\n\ninformative_beta_match\n\n[1] 19.68976\n\n##Informative Prior from Announcer\nalphas &lt;- seq(0.01, 1000, length.out = 2000) \nbetas &lt;- alphas / 3\n\n\n\ntarget_prob &lt;- 0.04\nprob_less_0.7 &lt;- pbeta(0.70, alphas, betas)\n\ntibble(alphas, betas, prob_less_0.7) |&gt;\n  mutate(close_to_target = abs(prob_less_0.7 - target_prob)) |&gt;\n  filter(close_to_target == min(close_to_target))\n\n# A tibble: 1 × 4\n  alphas betas prob_less_0.7 close_to_target\n   &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;           &lt;dbl&gt;\n1   182.  60.5        0.0400       0.0000205\n##When we created our informative priors, we assumed independence of points, ie\n##If Nadal wins two points in a row with his serve, the next serve was indepedent ##if the previous two or that all his opponents are of equal skill level\n## We are assuming game lengths are the same, like some tennis matches, \n## Nadal will serve a lot more\nlibrary(tidyverse)\n\nps &lt;- seq(0, 1, length.out = 1000)\n\nnoninformative_alpha &lt;- 1\nnoninformative_beta &lt;- 1\n\ninformative_alpha_match &lt;- 45.28644\ninformative_beta_match &lt;- 19.68976\n\ninformative_alpha_announce &lt;- 181.599\ninformative_beta_announce &lt;- 60.53299\n\nnoninformative_prior &lt;- dbeta(ps, noninformative_alpha, noninformative_beta)\ninformative_prior &lt;- dbeta(ps, informative_alpha_match, informative_beta_match)\ninformative_prior_announce &lt;- dbeta(ps, informative_alpha_announce, informative_beta_announce)\n\nprior_plot &lt;- tibble(ps, noninformative_prior, informative_prior, informative_prior_announce) |&gt;\n  pivot_longer(2:4, names_to = \"prior_type\", values_to = \"density\") |&gt;\n  mutate(prior_type = recode(prior_type,\n                             \"noninformative_prior\" = \"Non-informative Prior\",\n                             \"informative_prior\" = \"Informative Prior (Match Data)\",\n                             \"informative_prior_announce\" = \"Informative Prior (Announcer)\"))\n\nggplot(data = prior_plot, aes(x = ps, y = density, colour = prior_type)) +\n  geom_line() +\n  scale_colour_viridis_d(end = 0.9) +\n  theme_minimal() +\n  labs(x = \"p (Probability Nadal wins point on serve)\",\n       y = \"Density\",\n       colour = \"Prior Type\",\n       title = \"Three Prior Distributions for Nadal's Serve Performance\")\nps &lt;- seq(0, 1, length.out = 1000)\n\nnoninformative_alpha &lt;- 1+56\nnoninformative_beta &lt;- 84 -56+1\n\ninformative_alpha_match &lt;- 45.28644 +56\ninformative_beta_match &lt;- 84-56+19.68976\n\ninformative_alpha_announce &lt;- 181.599+56\ninformative_beta_announce &lt;- 84-56+60.53299\n\nnoninformative_post &lt;- dbeta(ps, noninformative_alpha, noninformative_beta)\ninformative_post &lt;- dbeta(ps, informative_alpha_match, informative_beta_match)\ninformative_post_announce &lt;- dbeta(ps, informative_alpha_announce, informative_beta_announce)\n\npost_plot &lt;- tibble(ps, noninformative_post, informative_post, informative_post_announce) |&gt;\n  pivot_longer(2:4, names_to = \"prior_type\", values_to = \"density\") |&gt;\n  mutate(prior_type = recode(prior_type,\n                             \"noninformative_prior\" = \"Non-informative Prior\",\n                             \"informative_prior\" = \"Informative Prior (Match Day)\",\n                             \"informative_prior_announce\" = \"Informative Prior (Announcer)\"))\n\nggplot(data = post_plot, aes(x = ps, y = density, colour = prior_type)) +\n  geom_line(size=1.2) +\n  scale_colour_viridis_d(end = 0.9) +\n  theme_minimal() +\n  labs(x = \"p (Probability Nadal wins point on serve)\",\n       y = \"Density\",\n       colour = \"Prior Type\",\n       title = \"Three Posterior Distributions for Nadal's Serve Performance\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\nnoninformative_alpha &lt;- 1+56\nnoninformative_beta &lt;- 84 -56+1 \n\ninformative_alpha_match &lt;- 45.28644 +56\ninformative_beta_match &lt;- 84-56+19.68976\n\ninformative_alpha_announce &lt;- 181.599+56\ninformative_beta_announce &lt;- 84-56+60.53299\n\n\nmean_noninformative &lt;- 57/ (57+29)\nmean_informative &lt;- 101.28644/ (101.28644+ 47.68976)\nmean_informative_announce &lt;- 237.599/(88.53299+237.599)\n\nmean_noninformative\n\n[1] 0.6627907\n\nmean_informative\n\n[1] 0.6798834\n\nmean_informative_announce\n\n[1] 0.7285363\n\n##NonInformative\nqbeta(0.05,57,29)\n\n[1] 0.5772453\n\nqbeta(0.95,57,29)\n\n[1] 0.7440061\n\n##Informative\nqbeta(0.05,101.28644,47.68976)\n\n[1] 0.6158464\n\nqbeta(0.95,101.28644,47.68976)\n\n[1] 0.7411652\n\n##Informative_Announce\nqbeta(0.05,237.599,88.53299)\n\n[1] 0.6873017\n\nqbeta(0.95,237.599,88.53299)\n\n[1] 0.7681751\n## (1)The credible interval for noninformative (0.577;0.744)\n## (2)The credible interval for informative_novak_stats (0.6158;0.7412)\n## (3)The credible intervals for informative_announce (0.687,0.768)\n\n## The posteriors differ because the priors have different assumptions:\n## The noninformative is driven simply by the data\n## The match data informative one is based on match data and the data\n## The announcer posterior has a very strong prior so may be overpowering \n## the data somewhat\n\n##The variance of the noninformative is the highest as shown by the graph\n\n##I would choose the informative posterior(2) using the match stats for the prior\n##The reason I have chosen this one is I believe the announcers one is too narrow\n## Nadal is the greatest tennis player on clay, so therefore he will have an\n## extremely high serve win ratio but the intial question was, what will his win \n## serve ratio be against Novak, who is an incredible tennis player too.\n## The announcer posterior is over confident in Nadal's serve when playing Novak,\n## as we assumed independence and that every opponent is equal but not every \n## opponent is equal. I feel that the announcer one is overdriven by the prior.\n\n## We do not lose much on an credible interval when selecting (2) over (3) and \n## looking at the density graphs, I feel that the little moved less graph is \n## more reflective of when Nadal plays Novak\n\n## What I found from this project is that a posterior can be overdriven by\n## a prior if the prior is too strong, What this does is make the density too \n## high and overstated by the prior. I learned that the smallest credible \n## interval isnt always the best. You have to consider a lot of factors to choose\n## the best predictor"
  },
  {
    "objectID": "posts/06-reflect/index.html",
    "href": "posts/06-reflect/index.html",
    "title": "Mini-Reflect",
    "section": "",
    "text": "Question 1\nThe first mini project looks at the standard error and expected value of different distributions like the uniform, normal, and beta distributions for example. The project connects to the concept of sampling distributions by showing how the max and min values behave across samples with different probability distributions. This project shows us how choosing the correct distribution to use influences the variability in all the different statistics we would create from the sample. In the larger scheme of things, this mini-project showed you can apply sampling distributions to more than the sample mean, which is what most people use sampling for. Mini-project 2 connects to the concept of estimations. If we are doing hypothesis tests or confidence intervals, we will most likely be looking at data from samples. Throughout the course we were looking at bias and variance and in this mini-project I explained the idea of bias, that the tree I sampled the number of troops from may be biased.\nMini-project 3 is looking at simulating thousands of sample proportions p from a binomial distribution, creating CIs, and then measuring the coverage rate and average width. The project connects with the class for example n=5 and p=0.1, the coverage rate is only 40%, so the true proportion p is contained only in 40% of the intervals. This connects to the class as it shows that even if we simulate thousands of times, a small sample leads to inaccurate and unreliable confidence intervals. The project shows that simply applying formulas and running thousands of simulations does not overcome the limitations of a small sample. This is a big idea of the whole class, the idea of central limit thereon and normally distributed.\nMini-project 4 connects to the content of the course by outlining another method for inference. This project highlights in terms of Bayesian inference, that without strong prior knowledge, inference should more lean on the observed data. This project illustrates how a prior can skew a distribution disproportionally. This connects to the content of the course by explaining that sometimes having too small of a credible interval is always the best as they misrepresent the uncertainty that we actually have.\nMini-project 5 connects to the class by looking into whether the p-value should be the be-all and end-all of making decisions in terms of hypothesis tests. If we use a significance level of 0.05 and the p-value comes out at 0.1, we simply say, we do not have enough evidence to reject Ho and that is the end of whether something affects another. What we learned in class is p-value is the indicator of the final word but mini-project 5 shows we should look at more things like context, where the data comes from, how its cleaned and used, and asses what effect this research will have.\nQuestion 2\nMini-project 3 and mini-project 4 connect in that they both explain how confidence intervals and credible intervals can misrepresent information and people can come to the wrong conclusions due to that fact. In the Bayesian mini-project we saw that we had an extremely tight credible interval which is often good but in this case, the prior was too strong which made the CI too overly confident of Nadal’s performance against Novak. Mini-project 3 with n=5 and p=0.1, a confidence interval will be created and someone who doesn’t know much about statistics may use that confidence interval to make decisions. Once again, the true proportion is only captured 40% of the time, so it would not be wise to use this CI. The connection between these two is a misrepresentation of results and the negative effects of that.\nMini-projects 2 and 5 related to the coursework by using the modern approach to teaching math stat. By writing a story with definitions and terms, you are having to not just do the math but understand it which is what the course is trying to achieve. In my mini-project 2, I explained the missile strike rate using a binomial distribution, and this is related to the coursework goal of being able to explain ideas from the class, not just do the math. In Mini-project 5, we had to explain our thoughts on the topic of p values. The two mini-projects relate in that a math stat course should prepare you for more than just doing math, but should prepare you to explain concepts or debate against ideas, using a math stat background.\nMini-project 2 and mini-project 4 both look at the idea of statistical inference from a frequentist and Bayesian point of view. In my Vietnam story, we calculated enemy number based on the number of troops behind two trees to find out how many there were in total. Mini project 4 uses a Bayesian approach and we use a prior and create a posterior after we have observed data. The Vietnam project estimates with observed data alone whereas the Nadal project uses prior information and observed data. Both projects look at how we incorporate uncertainty when estimating. After we have looked at where the p-value comes from and used p values a lot, mini-project 5 serves as warning to, us future possible statisticians.\nQuestion 3\nMy biggest takeaway is that statistical tools have their faults and you really have to look at the methods of research way more clearly to know if you can trust the research and whether you should make decisions based on someone else research. Two examples are a researcher could use a Bayesian approach and make their prior so strong as to make their research prove a point or a researcher could create confidence intervals from too small of a sample which misrepresents a conclusion. Researchers could also manipulate it to reach a certain p-value. The takeaway is that, when looking at research take more time to read the methods and less time reading the conclusion, if you want to know if it is worth making decisions from.\nI personally really enjoyed the mini-projects as it gave me some time to do repeated processes and see what was happening. When we would have to change small things of code, many times in different places, it really helped me understand the code and then the concepts as I am someone who needs to repeat tasks to fully understand it."
  }
]