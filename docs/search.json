[
  {
    "objectID": "posts/06-reflect/index.html",
    "href": "posts/06-reflect/index.html",
    "title": "Mini-Reflect",
    "section": "",
    "text": "Question 1\nThe first mini project looks at the standard error and expected value of different distributions like the uniform, normal, and beta distributions for example. The project connects to the concept of sampling distributions by showing how the max and min values behave across samples with different probability distributions. This project shows us how choosing the correct distribution to use influences the variability in all the different statistics we would create from the sample. In the larger scheme of things, this mini-project showed you can apply sampling distributions to more than the sample mean, which is what most people use sampling for. Mini-project 2 connects to the concept of estimations. If we are doing hypothesis tests or confidence intervals, we will most likely be looking at data from samples. Throughout the course we were looking at bias and variance and in this mini-project I explained the idea of bias, that the tree I sampled the number of troops from may be biased.\nMini-project 3 is looking at simulating thousands of sample proportions p from a binomial distribution, creating CIs, and then measuring the coverage rate and average width. The project connects with the class for example n=5 and p=0.1, the coverage rate is only 40%, so the true proportion p is contained only in 40% of the intervals. This connects to the class as it shows that even if we simulate thousands of times, a small sample leads to inaccurate and unreliable confidence intervals. The project shows that simply applying formulas and running thousands of simulations does not overcome the limitations of a small sample. This is a big idea of the whole class, the idea of central limit thereon and normally distributed.\nMini-project 4 connects to the content of the course by outlining another method for inference. This project highlights in terms of Bayesian inference, that without strong prior knowledge, inference should more lean on the observed data. This project illustrates how a prior can skew a distribution disproportionally. This connects to the content of the course by explaining that sometimes having too small of a credible interval is always the best as they misrepresent the uncertainty that we actually have.\nMini-project 5 connects to the class by looking into whether the p-value should be the be-all and end-all of making decisions in terms of hypothesis tests. If we use a significance level of 0.05 and the p-value comes out at 0.1, we simply say, we do not have enough evidence to reject Ho and that is the end of whether something affects another. What we learned in class is p-value is the indicator of the final word but mini-project 5 shows we should look at more things like context, where the data comes from, how its cleaned and used, and asses what effect this research will have.\nQuestion 2\nMini-project 3 and mini-project 4 connect in that they both explain how confidence intervals and credible intervals can misrepresent information and people can come to the wrong conclusions due to that fact. In the Bayesian mini-project we saw that we had an extremely tight credible interval which is often good but in this case, the prior was too strong which made the CI too overly confident of Nadal’s performance against Novak. Mini-project 3 with n=5 and p=0.1, a confidence interval will be created and someone who doesn’t know much about statistics may use that confidence interval to make decisions. Once again, the true proportion is only captured 40% of the time, so it would not be wise to use this CI. The connection between these two is a misrepresentation of results and the negative effects of that.\nMini-projects 2 and 5 related to the coursework by using the modern approach to teaching math stat. By writing a story with definitions and terms, you are having to not just do the math but understand it which is what the course is trying to achieve. In my mini-project 2, I explained the missile strike rate using a binomial distribution, and this is related to the coursework goal of being able to explain ideas from the class, not just do the math. In Mini-project 5, we had to explain our thoughts on the topic of p values. The two mini-projects relate in that a math stat course should prepare you for more than just doing math, but should prepare you to explain concepts or debate against ideas, using a math stat background.\nMini-project 2 and mini-project 4 both look at the idea of statistical inference from a frequentist and Bayesian point of view. In my Vietnam story, we calculated enemy number based on the number of troops behind two trees to find out how many there were in total. Mini project 4 uses a Bayesian approach and we use a prior and create a posterior after we have observed data. The Vietnam project estimates with observed data alone whereas the Nadal project uses prior information and observed data. Both projects look at how we incorporate uncertainty when estimating. After we have looked at where the p-value comes from and used p values a lot, mini-project 5 serves as warning to, us future possible statisticians.\nQuestion 3\nMy biggest takeaway is that statistical tools have their faults and you really have to look at the methods of research way more clearly to know if you can trust the research and whether you should make decisions based on someone else research. Two examples are a researcher could use a Bayesian approach and make their prior so strong as to make their research prove a point or a researcher could create confidence intervals from too small of a sample which misrepresents a conclusion. Researchers could also manipulate it to reach a certain p-value. The takeaway is that, when looking at research take more time to read the methods and less time reading the conclusion, if you want to know if it is worth making decisions from.\nI personally really enjoyed the mini-projects as it gave me some time to do repeated processes and see what was happening. When we would have to change small things of code, many times in different places, it really helped me understand the code and then the concepts as I am someone who needs to repeat tasks to fully understand it."
  },
  {
    "objectID": "posts/03-Mini/index.html",
    "href": "posts/03-Mini/index.html",
    "title": "Mini-Project-3",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tibble)\n\n\np=0.1\n\ngenerate_samp_prop &lt;- function(n,p) {\n  \n  x &lt;- rbinom(1, n, p) # randomly generate number of successes for the sample\n  \n  ## number of successes divided by sample size\n  phat &lt;- x / n\n  lb &lt;-phat -2.132*sqrt(phat* (1-phat)/n)\n  ub &lt;-phat +2.132*sqrt(phat* (1-phat)/n)\n  \n  prop_df &lt;-tibble(phat,lb,ub)\n  return(prop_df)\n}\ngenerate_samp_prop(n=5, p=0.1)\n\n# A tibble: 1 × 3\n   phat     lb    ub\n  &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1   0.2 -0.181 0.581\n\nn_sim &lt;-5000\n\nprop_ci_df &lt;-map(1:n_sim, \\ (i) generate_samp_prop(n=5, p =0.1)) |&gt; bind_rows()\n\nprop_ci_df &lt;- prop_ci_df |&gt; mutate(ci_width = ub - lb,\n                                   ci_cover_ind = if_else(p &gt; lb & p &lt; ub,\n                                                          true = 1, \n                                                          false = 0))\n\nprop_ci_df |&gt; summarise(avg_width = mean(ci_width),\n                        coverage_rate = mean(ci_cover_ind))\n\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.329         0.402\n\n\n\np=0.1\ngenerate_samp_prop &lt;- function(n,p) {\n  \n  x &lt;- rbinom(1, n, p) # randomly generate number of successes for the sample\n  \n  ## number of successes divided by sample size\n  phat &lt;- x / n\n  lb &lt;-phat -2.132*sqrt(phat* (1-phat)/n)\n  ub &lt;-phat +2.132*sqrt(phat* (1-phat)/n)\n  \n  prop_df &lt;-tibble(phat,lb,ub)\n  return(prop_df)\n}\ngenerate_samp_prop(n=5, p=0.1)\n\n# A tibble: 1 × 3\n   phat    lb    ub\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0     0     0\n\nn_sim &lt;-5000\n\nprop_ci_df &lt;-map(1:n_sim, \\ (i) generate_samp_prop(n=5, p =0.1)) |&gt; bind_rows()\n\nprop_ci_df &lt;- prop_ci_df |&gt; mutate(ci_width = ub - lb,\n                                   ci_cover_ind = if_else(p &gt; lb & p &lt; ub,\n                                                          true = 1, \n                                                          false = 0))\n\nprop_ci_df |&gt; summarise(avg_width = mean(ci_width),\n                        coverage_rate = mean(ci_cover_ind))\n\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.333         0.409\n\n\n\np=0.55\ngenerate_samp_prop &lt;- function(n,p) {\n  \n  x &lt;- rbinom(1, n, p) # randomly generate number of successes for the sample\n  \n  ## number of successes divided by sample size\n  phat &lt;- x / n\n  lb &lt;-phat -2.132*sqrt(phat* (1-phat)/n)\n  ub &lt;-phat +2.132*sqrt(phat* (1-phat)/n)\n  \n  prop_df &lt;-tibble(phat,lb,ub)\n  return(prop_df)\n}\ngenerate_samp_prop(n=5, p=0.55)\n\n# A tibble: 1 × 3\n   phat    lb    ub\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1   0.8 0.419  1.18\n\nn_sim &lt;-5000\n\nprop_ci_df &lt;-map(1:n_sim, \\ (i) generate_samp_prop(n=5, p =0.55)) |&gt; bind_rows()\n\nprop_ci_df &lt;- prop_ci_df |&gt; mutate(ci_width = ub - lb,\n                                   ci_cover_ind = if_else(p &gt; lb & p &lt; ub,\n                                                          true = 1, \n                                                          false = 0))\n\nprop_ci_df |&gt; summarise(avg_width = mean(ci_width),\n                        coverage_rate = mean(ci_cover_ind))\n\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.817         0.934\n\n\n\np=0.1\ngenerate_samp_prop &lt;- function(n,p) {\n  \n  x &lt;- rbinom(1, n, p) # randomly generate number of successes for the sample\n  \n  ## number of successes divided by sample size\n  phat &lt;- x / n\n  lb &lt;-phat -1.684*sqrt(phat* (1-phat)/n)\n  ub &lt;-phat +1.6842*sqrt(phat* (1-phat)/n)\n  \n  prop_df &lt;-tibble(phat,lb,ub)\n  return(prop_df)\n}\ngenerate_samp_prop(n=40, p=0.1)\n\n# A tibble: 1 × 3\n   phat       lb    ub\n  &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1  0.05 -0.00803 0.108\n\nn_sim &lt;-5000\n\nprop_ci_df &lt;-map(1:n_sim, \\ (i) generate_samp_prop(n=40, p =0.1)) |&gt; bind_rows()\n\nprop_ci_df &lt;- prop_ci_df |&gt; mutate(ci_width = ub - lb,\n                                   ci_cover_ind = if_else(p &gt; lb & p &lt; ub,\n                                                          true = 1, \n                                                          false = 0))\n\nprop_ci_df |&gt; summarise(avg_width = mean(ci_width),\n                        coverage_rate = mean(ci_cover_ind))\n\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.152         0.902\n\n\n\np=0.55\ngenerate_samp_prop &lt;- function(n,p) {\n  \n  x &lt;- rbinom(1, n, p) # randomly generate number of successes for the sample\n  \n  ## number of successes divided by sample size\n  phat &lt;- x / n\n  lb &lt;-phat -1.684*sqrt(phat* (1-phat)/n)\n  ub &lt;-phat +1.6842*sqrt(phat* (1-phat)/n)\n  \n  prop_df &lt;-tibble(phat,lb,ub)\n  return(prop_df)\n}\ngenerate_samp_prop(n=40, p=0.55)\n\n# A tibble: 1 × 3\n   phat    lb    ub\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 0.575 0.443 0.707\n\nn_sim &lt;-5000\n\nprop_ci_df &lt;-map(1:n_sim, \\ (i) generate_samp_prop(n=40, p =0.55)) |&gt; bind_rows()\n\nprop_ci_df &lt;- prop_ci_df |&gt; mutate(ci_width = ub - lb,\n                                   ci_cover_ind = if_else(p &gt; lb & p &lt; ub,\n                                                          true = 1, \n                                                          false = 0))\n\nprop_ci_df |&gt; summarise(avg_width = mean(ci_width),\n                        coverage_rate = mean(ci_cover_ind))\n\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.261         0.886\n\n\n\np=0.1\ngenerate_samp_prop &lt;- function(n,p) {\n  \n  x &lt;- rbinom(1, n, p) # randomly generate number of successes for the sample\n  \n  ## number of successes divided by sample size\n  phat &lt;- x / n\n  lb &lt;-phat -1.645*sqrt(phat* (1-phat)/n)\n  ub &lt;-phat +1.645*sqrt(phat* (1-phat)/n)\n  \n  prop_df &lt;-tibble(phat,lb,ub)\n  return(prop_df)\n}\ngenerate_samp_prop(n=300, p=0.1)\n\n# A tibble: 1 × 3\n    phat     lb    ub\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1 0.0967 0.0686 0.125\n\nn_sim &lt;-5000\n\nprop_ci_df &lt;-map(1:n_sim, \\ (i) generate_samp_prop(n=300, p =0.1)) |&gt; bind_rows()\n\nprop_ci_df &lt;- prop_ci_df |&gt; mutate(ci_width = ub - lb,\n                                   ci_cover_ind = if_else(p &gt; lb & p &lt; ub,\n                                                          true = 1, \n                                                          false = 0))\n\nprop_ci_df |&gt; summarise(avg_width = mean(ci_width),\n                        coverage_rate = mean(ci_cover_ind))\n\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      &lt;dbl&gt;         &lt;dbl&gt;\n1    0.0567         0.888\n\n\n\np=0.55\ngenerate_samp_prop &lt;- function(n,p) {\n  \n  x &lt;- rbinom(1, n, p) # randomly generate number of successes for the sample\n  \n  ## number of successes divided by sample size\n  phat &lt;- x / n\n  lb &lt;-phat -1.645*sqrt(phat* (1-phat)/n)\n  ub &lt;-phat +1.645*sqrt(phat* (1-phat)/n)\n  \n  prop_df &lt;-tibble(phat,lb,ub)\n  return(prop_df)\n}\ngenerate_samp_prop(n=300, p=0.55)\n\n# A tibble: 1 × 3\n   phat    lb    ub\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  0.57 0.523 0.617\n\nn_sim &lt;-5000\n\nprop_ci_df &lt;-map(1:n_sim, \\ (i) generate_samp_prop(n=300, p =0.55)) |&gt; bind_rows()\n\nprop_ci_df &lt;- prop_ci_df |&gt; mutate(ci_width = ub - lb,\n                                   ci_cover_ind = if_else(p &gt; lb & p &lt; ub,\n                                                          true = 1, \n                                                          false = 0))\n\nprop_ci_df |&gt; summarise(avg_width = mean(ci_width),\n                        coverage_rate = mean(ci_cover_ind))\n\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      &lt;dbl&gt;         &lt;dbl&gt;\n1    0.0943         0.892\n\n\n|  |         | $n = $5 | $n = $40 | $n = $300 |\n|:----:|:-----------------:|:-------------:|:------------:|:------------:|\n| $p = 0.1$   | Coverage Rate       | 0.4   |  0.9072     |  0.8838         |\n| $p = 0.55$   | Coverage Rate       |0.93     |0.8846   | 0.897    |\n|    |                     |               |              |              |\n| $p = 0.1$    | Average Width        |0.332   |  0.152  |   0.057       |\n| $p = 0.55$    | Average Width        |0.817  |  0.262     |  0.094      |\n\n\n: Table of Results {.striped .hover}"
  },
  {
    "objectID": "posts/01-simulation/index.html",
    "href": "posts/01-simulation/index.html",
    "title": "index",
    "section": "",
    "text": "##Jy lyk mooi"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Final_Portfolio_Dean",
    "section": "",
    "text": "Mini-Project-1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nindex\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMini Project 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMini-Project-3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMini-Project-4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMini-Reflect\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/01-Sim/index.html",
    "href": "posts/01-Sim/index.html",
    "title": "Dean Brooker Mini-Project1",
    "section": "",
    "text": "library(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nn &lt;- 5            # sample size\nmu &lt;- 10          # population mean\nsigma &lt;- 2        # population standard deviation\n\ngenerate_samp_min &lt;- function(mu, sigma, n) {\n  \n  single_sample &lt;- rnorm(n, mu, sigma)\n  sample_min &lt;- min(single_sample)\n  \n  return(sample_min)\n}\n\n## test function once:\ngenerate_samp_min(mu = mu, sigma = sigma, n = n)\n\n[1] 8.570612\n\nnsim &lt;- 5000      # number of simulations\n\n## code to map through the function. \n## the \\(i) syntax says to just repeat the generate_samp_mean function\n## nsim times\nmins &lt;- map_dbl(1:nsim, \\(i) generate_samp_min(mu = mu, sigma = sigma, n = n))\n\n## print some of the 5000 means\n## each number represents the sample mean from __one__ sample.\nmins_df &lt;- tibble(mins)\nmins_df\n\n# A tibble: 5,000 × 1\n    mins\n   &lt;dbl&gt;\n 1  9.37\n 2  7.19\n 3 11.3 \n 4  9.11\n 5  6.45\n 6  9.77\n 7  6.89\n 8  7.30\n 9  8.69\n10  5.90\n# ℹ 4,990 more rows\n\nggplot(data = mins_df, aes(x = mins)) +\n  geom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Mins\",\n       title = paste(\"Sampling Distribution of the \\nSample Mean when n =\", n))\n\n\n\n\n\n\n\nmins_df |&gt;\n  summarise(mean_samp_dist = mean(mins),\n            var_samp_dist = var(mins),\n            sd_samp_dist = sd(mins))\n\n# A tibble: 1 × 3\n  mean_samp_dist var_samp_dist sd_samp_dist\n           &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1           7.66          1.74         1.32\n\n#\nn &lt;- 5            # sample size\nmu &lt;- 10          # population mean\nsigma &lt;- 2        # population standard deviation\n\ngenerate_samp_max &lt;- function(mu, sigma, n) {\n  \n  single_sample &lt;- rnorm(n, mu, sigma)\n  sample_max &lt;- max(single_sample)\n  \n  return(sample_max)\n}\n\n## test function once:\ngenerate_samp_max(mu = mu, sigma = sigma, n = n)\n\n[1] 11.78113\n\nnsim &lt;- 5000      # number of simulations\n\n## code to map through the function. \n## the \\(i) syntax says to just repeat the generate_samp_mean function\n## nsim times\nmaxs &lt;- map_dbl(1:nsim, \\(i) generate_samp_max(mu = mu, sigma = sigma, n = n))\n\n## print some of the 5000 means\n## each number represents the sample mean from __one__ sample.\nmaxs_df &lt;- tibble(maxs)\nmaxs_df\n\n# A tibble: 5,000 × 1\n    maxs\n   &lt;dbl&gt;\n 1  13.1\n 2  11.6\n 3  11.8\n 4  13.3\n 5  11.5\n 6  11.2\n 7  13.1\n 8  12.2\n 9  14.6\n10  13.6\n# ℹ 4,990 more rows\n\nggplot(data = maxs_df, aes(x = maxs)) +\n  geom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Means\",\n       title = paste(\"Sampling Distribution of the \\nSample Mean when n =\", n))\n\n\n\n\n\n\n\nmaxs_df |&gt;\n  summarise(mean_samp_dist = mean(maxs),\n            var_samp_dist = var(maxs),\n            sd_samp_dist = sd(maxs))\n\n# A tibble: 1 × 3\n  mean_samp_dist var_samp_dist sd_samp_dist\n           &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1           12.3          1.84         1.36\n##uniform\nn &lt;- 5            # sample size\nmu &lt;- 10          # population mean\nsigma &lt;- 1.732    # population standard deviation (calculated by hand)\no1=7\no2=13\n\ngenerate_samp_min &lt;- function(n, mu, sigma) {\n  \n  single_sample &lt;- runif(n, o1,o2)\n  sample_min &lt;- min(single_sample)\n  \n  return(sample_min)\n}\n\n## test function once:\ngenerate_samp_min(mu = mu, sigma = sigma, n = n)\n\n[1] 9.249944\n\nnsim &lt;- 5000      # number of simulations\n\n## code to map through the function. \n## the \\(i) syntax says to just repeat the generate_samp_mean function\n## nsim times\nmins &lt;- map_dbl(1:nsim, \\(i) generate_samp_min(mu = mu, sigma = sigma, n = n))\n\n## print some of the 5000 means\n## each number represents the sample mean from __one__ sample.\nmins_df &lt;- tibble(mins)\nmins_df\n\n# A tibble: 5,000 × 1\n    mins\n   &lt;dbl&gt;\n 1  7.17\n 2 10.3 \n 3  8.08\n 4  7.37\n 5  7.40\n 6  7.15\n 7  7.19\n 8  9.09\n 9  7.22\n10  7.28\n# ℹ 4,990 more rows\n\nggplot(data = mins_df, aes(x = mins)) +\n  geom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Mins\",\n       title = paste(\"Sampling Distribution of the \\nSample Mean when n =\", n))\n\n\n\n\n\n\n\nmins_df |&gt;\n  summarise(mean_samp_dist = mean(mins),\n            var_samp_dist = var(mins),\n            sd_samp_dist = sd(mins))\n\n# A tibble: 1 × 3\n  mean_samp_dist var_samp_dist sd_samp_dist\n           &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1           8.00         0.722        0.850\nn &lt;- 5            # sample size\nmu &lt;- 10          # population mean\nsigma &lt;- 1.732       # population standard deviation\no1=7\no2=13\n\ngenerate_samp_max &lt;- function(mu, sigma, n) {\n  \n  single_sample &lt;- runif(n, o1,o2)\n  sample_max &lt;- max(single_sample)\n  \n  return(sample_max)\n}\n\n## test function once:\ngenerate_samp_max(mu = mu, sigma = sigma, n = n)\n\n[1] 12.86532\n\nnsim &lt;- 5000      # number of simulations\n\n## code to map through the function. \n## the \\(i) syntax says to just repeat the generate_samp_mean function\n## nsim times\nmaxs &lt;- map_dbl(1:nsim, \\(i) generate_samp_max(mu = mu, sigma = sigma, n = n))\n\n## print some of the 5000 means\n## each number represents the sample mean from __one__ sample.\nmaxs_df &lt;- tibble(maxs)\nmaxs_df\n\n# A tibble: 5,000 × 1\n    maxs\n   &lt;dbl&gt;\n 1 11.9 \n 2 12.8 \n 3 12.8 \n 4 12.5 \n 5 13.0 \n 6 12.7 \n 7 12.7 \n 8 10.0 \n 9  9.52\n10 12.4 \n# ℹ 4,990 more rows\n\nggplot(data = maxs_df, aes(x = maxs)) +\n  geom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Means\",\n       title = paste(\"Sampling Distribution of the \\nSample Mean when n =\", n))\n\n\n\n\n\n\n\nmaxs_df |&gt;\n  summarise(mean_samp_dist = mean(maxs),\n            var_samp_dist = var(maxs),\n            sd_samp_dist = sd(maxs))\n\n# A tibble: 1 × 3\n  mean_samp_dist var_samp_dist sd_samp_dist\n           &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1           12.0         0.694        0.833\n##exponential\nn &lt;- 5            # sample size\nmu &lt;- 2         # population mean\nsigma &lt;- 2    # population standard deviation (calculated by hand)\nlambda=0.5\n\ngenerate_samp_min &lt;- function(mu, sigma, n) {\n  \n  single_sample &lt;- rexp(n, lambda)\n  sample_min &lt;- min(single_sample)\n  \n  return(sample_min)\n}\n\n## test function once:\ngenerate_samp_min(mu = mu, sigma = sigma, n = n)\n\n[1] 1.016156\n\nnsim &lt;- 5000      # number of simulations\n\n## code to map through the function. \n## the \\(i) syntax says to just repeat the generate_samp_mean function\n## nsim times\nmins &lt;- map_dbl(1:nsim, \\(i) generate_samp_min(mu = mu, sigma = sigma, n = n))\n\n## print some of the 5000 means\n## each number represents the sample mean from __one__ sample.\nmins_df &lt;- tibble(mins)\nmins_df\n\n# A tibble: 5,000 × 1\n     mins\n    &lt;dbl&gt;\n 1 0.279 \n 2 0.461 \n 3 1.12  \n 4 0.0691\n 5 0.132 \n 6 0.264 \n 7 0.0515\n 8 1.05  \n 9 0.0178\n10 1.20  \n# ℹ 4,990 more rows\n\nggplot(data = mins_df, aes(x = mins)) +\n  geom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Mins\",\n       title = paste(\"Sampling Distribution of the \\nSample Mean when n =\", n))\n\n\n\n\n\n\n\nmins_df |&gt;\n  summarise(mean_samp_dist = mean(mins),\n            var_samp_dist = var(mins),\n            sd_samp_dist = sd(mins))\n\n# A tibble: 1 × 3\n  mean_samp_dist var_samp_dist sd_samp_dist\n           &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1          0.405         0.153        0.392\nn &lt;- 5            # sample size\nmu &lt;- 2         # population mean\nsigma &lt;- 2  # population standard deviation\nlambda=0.5\n\ngenerate_samp_max &lt;- function(mu, sigma, n) {\n  \n  single_sample &lt;- rexp(n, lambda)\n  sample_max &lt;- max(single_sample)\n  \n  return(sample_max)\n}\n\n## test function once:\ngenerate_samp_max(mu = mu, sigma = sigma, n = n)\n\n[1] 0.9294892\n\nnsim &lt;- 5000      # number of simulations\n\n## code to map through the function. \n## the \\(i) syntax says to just repeat the generate_samp_mean function\n## nsim times\nmaxs &lt;- map_dbl(1:nsim, \\(i) generate_samp_max(mu = mu, sigma = sigma, n = n))\n\n## print some of the 5000 means\n## each number represents the sample mean from __one__ sample.\nmaxs_df &lt;- tibble(maxs)\nmaxs_df\n\n# A tibble: 5,000 × 1\n    maxs\n   &lt;dbl&gt;\n 1  3.03\n 2  3.80\n 3  4.08\n 4  2.62\n 5  5.06\n 6  4.87\n 7  7.68\n 8  5.27\n 9  2.78\n10 11.3 \n# ℹ 4,990 more rows\n\nggplot(data = maxs_df, aes(x = maxs)) +\n  geom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Means\",\n       title = paste(\"Sampling Distribution of the \\nSample Mean when n =\", n))\n\n\n\n\n\n\n\nmaxs_df |&gt;\n  summarise(mean_samp_dist = mean(maxs),\n            var_samp_dist = var(maxs),\n            sd_samp_dist = sd(maxs))\n\n# A tibble: 1 × 3\n  mean_samp_dist var_samp_dist sd_samp_dist\n           &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1           4.56          5.74         2.40\n##beta\nn &lt;- 5            # sample size\nmu &lt;- 0.8         # population mean\nsigma &lt;- 0.1206    # population standard deviation (calculated by hand)\na=8\nb=2\n\ngenerate_samp_min &lt;- function(mu, sigma, n) {\n  \n  single_sample &lt;- rbeta(n,a,b)\n  sample_min &lt;- min(single_sample)\n  \n  return(sample_min)\n}\n\n## test function once:\ngenerate_samp_min(mu = mu, sigma = sigma, n = n)\n\n[1] 0.698269\n\nnsim &lt;- 5000      # number of simulations\n\n## code to map through the function. \n## the \\(i) syntax says to just repeat the generate_samp_mean function\n## nsim times\nmins &lt;- map_dbl(1:nsim, \\(i) generate_samp_min(mu = mu, sigma = sigma, n = n))\n\n## print some of the 5000 means\n## each number represents the sample mean from __one__ sample.\nmins_df &lt;- tibble(mins)\nmins_df\n\n# A tibble: 5,000 × 1\n    mins\n   &lt;dbl&gt;\n 1 0.773\n 2 0.487\n 3 0.640\n 4 0.696\n 5 0.709\n 6 0.837\n 7 0.628\n 8 0.812\n 9 0.708\n10 0.794\n# ℹ 4,990 more rows\n\nggplot(data = mins_df, aes(x = mins)) +\n  geom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Mins\",\n       title = paste(\"Sampling Distribution of the \\nSample Mean when n =\", n))\n\n\n\n\n\n\n\nmins_df |&gt;\n  summarise(mean_samp_dist = mean(mins),\n            var_samp_dist = var(mins),\n            sd_samp_dist = sd(mins))\n\n# A tibble: 1 × 3\n  mean_samp_dist var_samp_dist sd_samp_dist\n           &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1          0.646        0.0110        0.105\nn &lt;- 5            # sample size\nmu &lt;- 0.8         # population mean\nsigma &lt;- 0.1206    # population standard deviation (calculated by hand)\na=8\nb=2\n\ngenerate_samp_max &lt;- function(mu, sigma, n) {\n  \n  single_sample &lt;- rbeta(n,a,b)\n  sample_max &lt;- max(single_sample)\n  \n  return(sample_max)\n}\n\n## test function once:\ngenerate_samp_max(mu = mu, sigma = sigma, n = n)\n\n[1] 0.9628284\n\nnsim &lt;- 5000      # number of simulations\n\n## code to map through the function. \n## the \\(i) syntax says to just repeat the generate_samp_mean function\n## nsim times\nmaxs &lt;- map_dbl(1:nsim, \\(i) generate_samp_max(mu = mu, sigma = sigma, n = n))\n\n## print some of the 5000 means\n## each number represents the sample mean from __one__ sample.\nmaxs_df &lt;- tibble(maxs)\nmaxs_df\n\n# A tibble: 5,000 × 1\n    maxs\n   &lt;dbl&gt;\n 1 0.993\n 2 0.899\n 3 0.851\n 4 0.883\n 5 0.903\n 6 0.927\n 7 0.978\n 8 0.944\n 9 0.983\n10 0.949\n# ℹ 4,990 more rows\n\nggplot(data = maxs_df, aes(x = maxs)) +\n  geom_histogram(colour = \"deeppink4\", fill = \"deeppink1\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Means\",\n       title = paste(\"Sampling Distribution of the \\nSample Mean when n =\", n))\n\n\n\n\n\n\n\nmaxs_df |&gt;\n  summarise(mean_samp_dist = mean(maxs),\n            var_samp_dist = var(maxs),\n            sd_samp_dist = sd(maxs))\n\n# A tibble: 1 × 3\n  mean_samp_dist var_samp_dist sd_samp_dist\n           &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;\n1          0.921       0.00213       0.0461\n##min\nn &lt;- 5\n## CHANGE 0 and 3 to represent where you want your graph to start and end\n## on the x-axis\nx &lt;- seq(0, 10, length.out = 1000)\n## CHANGE to be the pdf you calculated. Note that, as of now, \n## this is not a proper density (it does not integrate to 1).\ndensity &lt;- 5 * (1 - (1 - exp(-0.5 * x))^4) * (0.5 * exp(-0.5 * x))\n\n\n\n## put into tibble and plot\nsamp_min_df &lt;- tibble(x, density)\nggplot(data = samp_min_df, aes(x = x, y = density)) +\n  geom_line() +\n  theme_minimal()\nn &lt;- 5\n## CHANGE 0 and 3 to represent where you want your graph to start and end\n## on the x-axis\nx &lt;- seq(0, 10, length.out = 1000)\n## CHANGE to be the pdf you calculated. Note that, as of now, \n## this is not a proper density (it does not integrate to 1).\ndensity &lt;- 5 * (1 - exp(-0.5 * x)^4)*(0.5 * exp(-0.5 * x))\n\n\n\n## put into tibble and plot\nsamp_min_df &lt;- tibble(x, density)\nggplot(data = samp_min_df, aes(x = x, y = density)) +\n  geom_line() +\n  theme_minimal()"
  },
  {
    "objectID": "posts/02-Story/index.html",
    "href": "posts/02-Story/index.html",
    "title": "Mini Project 2",
    "section": "",
    "text": "Dean Brooker\nMini Project\nA True Statistical American Hero\nThat is when it all changed. I changed it all.\nAfter graduation, in 1974, I decided to take some time off and travel around Vietnam. I arrived in Saigon, with my backpack, three shirts, and two pairs of pants. I had no plan.\nI heard a bomb explode, from far into the horizon. Then I heard another explosion. Then another. As the night went on, the bombs sounded as if they were getting closer to me. I remembered in school that sound travels at something like 300 meters a second so now wide awake, I decided to see how far the bombs were away from me. Way into the horizon, I would see a flash of light, and then I would count the amount of time it took for me to hear the blast. One…two…three… Fourteen seconds. Fourteen seconds multiplied by 300 meters means the blast was 4.2 kilometers away more or less. That was my estimate of how far the bombs were, away from me. After that, I could fall asleep, knowing the bombs were far away.\nThe next morning, I decided I would go for a hike in the jungle. The jungle was thick and the air was humid. After an hour of walking, I bumped into a man wearing camouflage clothing. He ran towards me and shouted, “You are in a war zone, get away”. I shouted back, “No, I am on holiday, nothing will stop my holiday.”\nThe man approached me and said, “I lost my platoon 20 minutes ago, I will never find them again !” I replied in a rather humorous tone, “Well we can find your platoon in no time.”\nI asked how fast the platoon walked. He said that they walk at a steady pace of 5 kilometers per hour. I explained that the marching speed was our parameter. The lost soldier said they were walking exactly due east. I simply explained that if they were twenty minutes ahead, a quarter of an hour, and moving at 5 km/h, they were 1.67km due east from us. We just need to walk a bit faster than 5km/h and we will catch them in no time.\nThe lone soldier said, “Did you just use an estimator to infer the location of my platoon based on the parameter of their marching speed?”\nI smirked and replied, “Yes, yes I did !”\nI joined the lone soldier and in no time, we found his platoon. When we arrived we found the platoon, in a gunfight with the\nOne soldier shouted, “I have no clue how many North Vietnamese soldiers there are?”\nI replied, “Well it would be easy to find out more or less how many there are !”\nThere were ten big trees, North Vietnamese soldiers were hiding behind. I snuck over our platoon’s cover and counted how many soldiers were behind two trees. I counted 30 behind two trees. Using my sample, we were up against 150 North Vietnamese soldiers.\nOne soldier shouted, “We have lost 2 soldiers in the last 30 minutes, and they have lost 30 soldiers in the last 30 minutes!”\nI shouted, “We must keep fighting! If we define X as the number of enemies we eliminate in a given time, it’s a random variable following a predictable trend! So far, our killing rate is 30 North Vietnamese every 30 minutes, therefore following this we will kill the remaining 120 soldiers in two hours!\nTwo hours and 20 minutes later, we had killed the entire opposing force.\nA frustrated soldier shouted, “You said it would take two hours!”:\nI explained, “Maybe the sample of the two trees was biased! “Maybe the two trees I checked had fewer soldiers than the others, meaning the original estimate of 150 soldiers could have been too low, maybe there were 180 soldiers. I didn’t account for variance either. Variance on the other hand refers to how much the number of enemies we eliminate fluctuates. Our kill rate of 30 per 30 minutes was based on a short sample, but in reality, some fighting waves are more intense than others. The randomness of battle means the actual elimination rate isn’t constant and we should have accounted for variance !”\nThe platoon of flabbergasted soldiers nodded and agreed.\nOne soldier asked, “Who are you? Why are you here?”\nI simply replied, “I am on holiday here in Vietnam, I am just a boy from South Africa!”\nA soldier shouted, “We may have won the battle, but we still need to get back to base!”\nI shouted, “I can help!\nThere were three routes: one along the riverbank, one through the thick of the jungle, and one over the mountain. Based on my reading of war novels, I knew there would be more enemy troops along the river route, as it was the easiest and closest to water. Based on my reading, I decided the jungle route gave us the most cover!\nI shouted, “Given the observed data, enemy patrol patterns, terrain difficulty, and past escape routes, the likelihood function suggests the jungle path has the highest probability of getting us back safely,”\nAfter three hours of walking, we arrived safely at the base. The one soldier said that I needed to meet General Johnson, as he would thank us for getting his troops back safely.\nI entered the war room and saw General Johnson playing a war game with die. I watched and worked out General Johnson was making mistakes!\n“You are basing your strategy on just a few dice rolls, that is not a reliable approach, what you need is a proper random sample !” A single roll is not enough but if you roll enough times, you get a better estimate of the true probabilities !”, I explained.\nOne general folded his arms and said, So you saying we need more data?”\nI nodded. “Exactly. A good estimator is consistent, meaning that as you increase the sample size, your results get closer to the true probabilities. If you roll enough times, your estimates of battlefield success rates will stabilize and reflect reality more accurately!”\nI sat in the room and continued to watch the generals play the war game.\nA soldier ran into the room and said, there are ten ships in the bay coming to attack, we need to shoot them down.\nThe general shouted, “Our missiles have an 80% success rate, so let’s just take 12 missiles to be the same, then surely we will sink all ten ships !”\nI replied, “Sadly that is wrong !”\nThe president of the US had just walked in and asked, “Who exactly said that and who are you !”\nI answered, “My name is Dean, I am on holiday!”\nI explained that we had a binomial problem on our hands. If the probability of success was 80% per missile, using a binomial distribution, with ten successes needed, we would need 16 missiles to ensure 99.99% success in shouting down the ten boats !”\nThe general shouted, “You heard the holidaymaker, grab 16 missiles”\nAfter this long day, I was becoming quite peckish. I thanked the general and president for having me and walked down the street in search of the nearest restaurant.\nTwo days later, I returned to the USA. When I landed at the US airport, I saw a headline in one of the newspapers that said, “True American Hero: Boy on holiday, saves lost soldier, wins a battle in the Vietnamese jungle, fixes US war game, and shoots down 10 North Vietnamese ships !”\nI thought to myself, what an incredible story. I grabbed the newspaper and read the story. It was incredible. I hope one day, I get to meet this boy!"
  },
  {
    "objectID": "posts/04-Bayez/index.html",
    "href": "posts/04-Bayez/index.html",
    "title": "Mini-Project",
    "section": "",
    "text": "##This project will attempt to answer the question of what is the probability\n##Nadla wins a point on his own serve against Novak Djokovic,at the French Open ?\n\n##To answer this question, we will use bayesian analysis where we will make some\n##informed assumptions paired with availiable data to create a probability that, \n##Nadal will wins a point on his own serve against Djokovic, at the French Open\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nps &lt;- seq(0.1, 200, length.out = 5000)\n\n\n##non-informed prior\n##beta(1,1)\nnoninformative_alpha &lt;- 1\nnoninformative_beta &lt;- 1\n\nnoninformative_prior &lt;- dbeta(ps,\n                              noninformative_alpha, noninformative_beta)\n\n##Informative Prior with Match Stats\nmatch_mean &lt;- 46 / 66\nmatch_se &lt;- 0.05657\nmatch_var &lt;- match_se^2\n\ntarget_mean &lt;- match_mean\ntarget_var &lt;- match_var\n\ninformative_alphas &lt;- seq(0.1, 200, length.out = 5000)\ninformative_betas &lt;- (informative_alphas * (1 - target_mean)) / target_mean\ninformative_vars &lt;- (informative_alphas * informative_betas) / ((informative_alphas + informative_betas)^2 * (informative_alphas + informative_betas + 1))\n\nparam_df &lt;- tibble(informative_alphas, informative_betas, informative_vars) |&gt;\n  mutate(dist_to_target = abs(informative_vars - target_var))\n\nbest_params &lt;- param_df |&gt; filter(dist_to_target == min(dist_to_target))\n\ninformative_alpha_match &lt;- best_params$informative_alphas\ninformative_beta_match &lt;- best_params$informative_betas\n\ninformative_alpha_match\n\n[1] 45.28644\n\ninformative_beta_match\n\n[1] 19.68976\n\n##Informative Prior from Announcer\nalphas &lt;- seq(0.01, 1000, length.out = 2000) \nbetas &lt;- alphas / 3\n\n\n\ntarget_prob &lt;- 0.04\nprob_less_0.7 &lt;- pbeta(0.70, alphas, betas)\n\ntibble(alphas, betas, prob_less_0.7) |&gt;\n  mutate(close_to_target = abs(prob_less_0.7 - target_prob)) |&gt;\n  filter(close_to_target == min(close_to_target))\n\n# A tibble: 1 × 4\n  alphas betas prob_less_0.7 close_to_target\n   &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;           &lt;dbl&gt;\n1   182.  60.5        0.0400       0.0000205\n##When we created our informative priors, we assumed independence of points, ie\n##If Nadal wins two points in a row with his serve, the next serve was indepedent ##if the previous two or that all his opponents are of equal skill level\n## We are assuming game lengths are the same, like some tennis matches, \n## Nadal will serve a lot more\nlibrary(tidyverse)\n\nps &lt;- seq(0, 1, length.out = 1000)\n\nnoninformative_alpha &lt;- 1\nnoninformative_beta &lt;- 1\n\ninformative_alpha_match &lt;- 45.28644\ninformative_beta_match &lt;- 19.68976\n\ninformative_alpha_announce &lt;- 181.599\ninformative_beta_announce &lt;- 60.53299\n\nnoninformative_prior &lt;- dbeta(ps, noninformative_alpha, noninformative_beta)\ninformative_prior &lt;- dbeta(ps, informative_alpha_match, informative_beta_match)\ninformative_prior_announce &lt;- dbeta(ps, informative_alpha_announce, informative_beta_announce)\n\nprior_plot &lt;- tibble(ps, noninformative_prior, informative_prior, informative_prior_announce) |&gt;\n  pivot_longer(2:4, names_to = \"prior_type\", values_to = \"density\") |&gt;\n  mutate(prior_type = recode(prior_type,\n                             \"noninformative_prior\" = \"Non-informative Prior\",\n                             \"informative_prior\" = \"Informative Prior (Match Data)\",\n                             \"informative_prior_announce\" = \"Informative Prior (Announcer)\"))\n\nggplot(data = prior_plot, aes(x = ps, y = density, colour = prior_type)) +\n  geom_line() +\n  scale_colour_viridis_d(end = 0.9) +\n  theme_minimal() +\n  labs(x = \"p (Probability Nadal wins point on serve)\",\n       y = \"Density\",\n       colour = \"Prior Type\",\n       title = \"Three Prior Distributions for Nadal's Serve Performance\")\nps &lt;- seq(0, 1, length.out = 1000)\n\nnoninformative_alpha &lt;- 1+56\nnoninformative_beta &lt;- 84 -56+1\n\ninformative_alpha_match &lt;- 45.28644 +56\ninformative_beta_match &lt;- 84-56+19.68976\n\ninformative_alpha_announce &lt;- 181.599+56\ninformative_beta_announce &lt;- 84-56+60.53299\n\nnoninformative_post &lt;- dbeta(ps, noninformative_alpha, noninformative_beta)\ninformative_post &lt;- dbeta(ps, informative_alpha_match, informative_beta_match)\ninformative_post_announce &lt;- dbeta(ps, informative_alpha_announce, informative_beta_announce)\n\npost_plot &lt;- tibble(ps, noninformative_post, informative_post, informative_post_announce) |&gt;\n  pivot_longer(2:4, names_to = \"prior_type\", values_to = \"density\") |&gt;\n  mutate(prior_type = recode(prior_type,\n                             \"noninformative_prior\" = \"Non-informative Prior\",\n                             \"informative_prior\" = \"Informative Prior (Match Day)\",\n                             \"informative_prior_announce\" = \"Informative Prior (Announcer)\"))\n\nggplot(data = post_plot, aes(x = ps, y = density, colour = prior_type)) +\n  geom_line(size=1.2) +\n  scale_colour_viridis_d(end = 0.9) +\n  theme_minimal() +\n  labs(x = \"p (Probability Nadal wins point on serve)\",\n       y = \"Density\",\n       colour = \"Prior Type\",\n       title = \"Three Posterior Distributions for Nadal's Serve Performance\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\nnoninformative_alpha &lt;- 1+56\nnoninformative_beta &lt;- 84 -56+1 \n\ninformative_alpha_match &lt;- 45.28644 +56\ninformative_beta_match &lt;- 84-56+19.68976\n\ninformative_alpha_announce &lt;- 181.599+56\ninformative_beta_announce &lt;- 84-56+60.53299\n\n\nmean_noninformative &lt;- 57/ (57+29)\nmean_informative &lt;- 101.28644/ (101.28644+ 47.68976)\nmean_informative_announce &lt;- 237.599/(88.53299+237.599)\n\nmean_noninformative\n\n[1] 0.6627907\n\nmean_informative\n\n[1] 0.6798834\n\nmean_informative_announce\n\n[1] 0.7285363\n\n##NonInformative\nqbeta(0.05,57,29)\n\n[1] 0.5772453\n\nqbeta(0.95,57,29)\n\n[1] 0.7440061\n\n##Informative\nqbeta(0.05,101.28644,47.68976)\n\n[1] 0.6158464\n\nqbeta(0.95,101.28644,47.68976)\n\n[1] 0.7411652\n\n##Informative_Announce\nqbeta(0.05,237.599,88.53299)\n\n[1] 0.6873017\n\nqbeta(0.95,237.599,88.53299)\n\n[1] 0.7681751\n## (1)The credible interval for noninformative (0.577;0.744)\n## (2)The credible interval for informative_novak_stats (0.6158;0.7412)\n## (3)The credible intervals for informative_announce (0.687,0.768)\n\n## The posteriors differ because the priors have different assumptions:\n## The noninformative is driven simply by the data\n## The match data informative one is based on match data and the data\n## The announcer posterior has a very strong prior so may be overpowering \n## the data somewhat\n\n##The variance of the noninformative is the highest as shown by the graph\n\n##I would choose the informative posterior(2) using the match stats for the prior\n##The reason I have chosen this one is I believe the announcers one is too narrow\n## Nadal is the greatest tennis player on clay, so therefore he will have an\n## extremely high serve win ratio but the intial question was, what will his win \n## serve ratio be against Novak, who is an incredible tennis player too.\n## The announcer posterior is over confident in Nadal's serve when playing Novak,\n## as we assumed independence and that every opponent is equal but not every \n## opponent is equal. I feel that the announcer one is overdriven by the prior.\n\n## We do not lose much on an credible interval when selecting (2) over (3) and \n## looking at the density graphs, I feel that the little moved less graph is \n## more reflective of when Nadal plays Novak\n\n## What I found from this project is that a posterior can be overdriven by\n## a prior if the prior is too strong, What this does is make the density too \n## high and overstated by the prior. I learned that the smallest credible \n## interval isnt always the best. You have to consider a lot of factors to choose\n## the best predictor"
  }
]